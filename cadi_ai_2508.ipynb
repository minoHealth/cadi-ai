{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4ce26f",
   "metadata": {},
   "source": [
    "# CADI AI Model Training Pipeline\n",
    "\n",
    "Clean modular training pipeline using external Python scripts.\n",
    "This notebook orchestrates the training process using `!python` commands to run our modular scripts.\n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Environment Setup** - Install dependencies and set paths\n",
    "2. **Dataset Preparation** - Create data.yaml and validate dataset\n",
    "3. **Model Training** - Train YOLO model with optimized settings\n",
    "4. **Evaluation** - Validate and test the trained model\n",
    "\n",
    "## Requirements:\n",
    "- Python scripts: `dataset_utils.py`, `train.py`\n",
    "- Configuration file: `config.yaml`\n",
    "- Dataset in proper YOLO format\n",
    "- GPU recommended for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca358976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58c182f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.9/86.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.5.1 which is incompatible.\n",
      "langchain-core 0.3.66 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m✅ Dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -U -q ultralytics roboflow opencv-python supervision PyYAML \"numpy <2\"\n",
    "\n",
    "# Import basic libraries\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"✅ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02186338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cadi-ai'...\n",
      "remote: Enumerating objects: 33, done.\u001b[K\n",
      "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
      "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
      "remote: Total 33 (delta 13), reused 27 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (33/33), 23.73 KiB | 3.95 MiB/s, done.\n",
      "Resolving deltas: 100% (13/13), done.\n",
      "🔧 Environment: kaggle\n",
      "📁 Project directory: /kaggle/working/cadi-ai\n",
      "💾 Working directory: /kaggle/working/cadi-training-2508\n"
     ]
    }
   ],
   "source": [
    "# Environment Configuration\n",
    "# Adjust these paths based on your environment (Kaggle, Colab, or local)\n",
    "\n",
    "# For Kaggle:\n",
    "if '/kaggle' in os.getcwd():\n",
    "    # This is where the repository will be cloned\n",
    "    PROJECT_DIR = '/kaggle/working/cadi-ai'\n",
    "    # Assumes the dataset is in /kaggle/input\n",
    "    DATASET_PATH = '/kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset'  # Update this\n",
    "    WORKING_DIR = '/kaggle/working/cadi-training-2508'\n",
    "    ENVIRONMENT = 'kaggle'\n",
    "\n",
    "# For Google Colab:\n",
    "elif '/content' in os.getcwd():\n",
    "    PROJECT_DIR = '/content/cadi-ai'\n",
    "    DATASET_PATH = '/content/dataset'  # Update this\n",
    "    WORKING_DIR = '/content/cadi-training-2508'\n",
    "    ENVIRONMENT = 'colab'\n",
    "\n",
    "# For local development:\n",
    "else:\n",
    "    PROJECT_DIR = r'c:\\Users\\Mecha Mino 5 Outlook\\Documents\\Mino Health AI labs\\cadi-ai'\n",
    "    DATASET_PATH = r'c:\\Users\\Mecha Mino 5 Outlook\\Documents\\Mino Health AI labs\\cadi-ai\\dataset'  # Update this\n",
    "    WORKING_DIR = r'c:\\Users\\Mecha Mino 5 Outlook\\Documents\\Mino Health AI labs\\cadi-ai\\training_outputs'\n",
    "    ENVIRONMENT = 'local'\n",
    "\n",
    "# Create working directory\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "\n",
    "# In Kaggle, we need to clone the repo first\n",
    "if ENVIRONMENT == 'kaggle':\n",
    "    !git clone https://github.com/minoHealth/cadi-ai.git \n",
    "    os.chdir(PROJECT_DIR)\n",
    "else:\n",
    "    os.chdir(PROJECT_DIR)\n",
    "\n",
    "print(f\"🔧 Environment: {ENVIRONMENT}\")\n",
    "print(f\"📁 Project directory: {os.getcwd()}\")\n",
    "print(f\"💾 Working directory: {WORKING_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8831e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Recreating data.yaml with CADI AI class names...\n",
      "🔍 Detected 3 classes from label files: [0, 1, 2]\n",
      "🎯 Using CADI AI class names: ['abiotic', 'disease', 'insect']\n",
      "⚠️  Dataset in read-only location, caching disabled\n",
      "✅ Created data.yaml at: /kaggle/working/cadi-ai/data.yaml\n",
      "📊 Dataset Validation Report\n",
      "========================================\n",
      "✅ train: 2813 images, 2813 labels\n",
      "✅ val  :  501 images,  501 labels\n",
      "✅ test :  340 images,  340 labels\n",
      "\n",
      "📈 Class Distribution:\n",
      "  abiotic   :  7762 objects ( 57.2%)\n",
      "  disease   :  3566 objects ( 26.3%)\n",
      "  insect    :  2236 objects ( 16.5%)\n",
      "\n",
      "📋 Summary:\n",
      "  Total Images: 3654\n",
      "  Total Labels: 3654\n",
      "  Total Objects: 13564\n",
      "  Classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Recreate data.yaml with improved class name detection\n",
    "print(\"🔄 Recreating data.yaml with CADI AI class names...\")\n",
    "\n",
    "!python dataset_utils.py --create-yaml {DATASET_PATH} --output-path {os.path.join(PROJECT_DIR, 'data.yaml')} --cache-dir {os.path.join(WORKING_DIR, 'cache')}\n",
    "\n",
    "# print(\"\\n📊 Re-validating with proper class names...\")\n",
    "!python dataset_utils.py --validate {os.path.join(PROJECT_DIR, 'data.yaml')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845c1de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Current data.yaml content:\n",
      "cache: false\n",
      "names:\n",
      "- abiotic\n",
      "- disease\n",
      "- insect\n",
      "nc: 3\n",
      "path: /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset\n",
      "test: /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/test/images\n",
      "train: /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/train/images\n",
      "val: /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/valid/images\n",
      "\n",
      "\n",
      "⚙️ Current config.yaml cache setting:\n",
      "cache: False\n",
      "\n",
      "📁 Cache directory: /kaggle/working/cadi-training-2508/cache\n",
      "📁 Project directory: /kaggle/working/cadi-ai\n",
      "📁 Working directory: /kaggle/working/cadi-training-2508\n"
     ]
    }
   ],
   "source": [
    "# Check current data.yaml and config.yaml cache settings\n",
    "print(\"📋 Current data.yaml content:\")\n",
    "data_yaml_path = os.path.join(PROJECT_DIR, 'data.yaml')\n",
    "if os.path.exists(data_yaml_path):\n",
    "    with open(data_yaml_path, 'r') as f:\n",
    "        data_content = f.read()\n",
    "        print(data_content)\n",
    "else:\n",
    "    print(\"❌ data.yaml not found\")\n",
    "\n",
    "print(\"\\n⚙️ Current config.yaml cache setting:\")\n",
    "config_yaml_path = os.path.join(PROJECT_DIR, 'config.yaml')\n",
    "if os.path.exists(config_yaml_path):\n",
    "    with open(config_yaml_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        cache_setting = config.get('cache', 'not set')\n",
    "        print(f\"cache: {cache_setting}\")\n",
    "else:\n",
    "    print(\"❌ config.yaml not found\")\n",
    "\n",
    "print(f\"\\n📁 Cache directory: {os.path.join(WORKING_DIR, 'cache')}\")\n",
    "print(f\"📁 Project directory: {PROJECT_DIR}\")\n",
    "print(f\"📁 Working directory: {WORKING_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eea6721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Finding optimal batch size for your hardware...\n",
      "This may take a few minutes as it tests different batch sizes.\n",
      "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Finding optimal batch size...\n",
      "🔍 Finding optimal batch size...\n",
      "  Testing batch size: 32\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo1\n",
      "Ultralytics 8.3.174 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=0, perspective=0.0, plots=False, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralyti\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   1413337  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
      "YOLO11m summary: 231 layers, 20,055,321 parameters, 20,055,305 gradients, 68.2 GFLOPs\n",
      "\n",
      "Transferred 643/649 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo1\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.2±0.3 ms, read: 25.5±10.1 MB/s, size: 100.5 KB)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_datas\u001b[0m\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/train is not writeable, cache not saved.\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.5±0.1 ms, read: 24.0±9.0 MB/s, size: 97.0 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset\u001b[0m\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/valid is not writeable, cache not saved.\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/1      15.5G      2.454      3.584      1.828         98        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        501       1715    0.00977     0.0181     0.0056    0.00155\n",
      "\n",
      "1 epochs completed in 0.038 hours.\n",
      "Optimizer stripped from runs/detect/train/weights/last.pt, 40.5MB\n",
      "Optimizer stripped from runs/detect/train/weights/best.pt, 40.5MB\n",
      "\n",
      "Validating runs/detect/train/weights/best.pt...\n",
      "Ultralytics 8.3.174 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,032,345 parameters, 0 gradients, 67.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        501       1715     0.0119     0.0168    0.00613    0.00159\n",
      "Speed: 0.2ms preprocess, 12.1ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "✅ Batch size 32 works!\n",
      "✅ Optimal batch size found: 32\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Find optimal batch size\n",
    "print(\"🔍 Finding optimal batch size for your hardware...\")\n",
    "print(\"This may take a few minutes as it tests different batch sizes.\")\n",
    "\n",
    "# We use the --find-batch argument from our updated train.py script\n",
    "!python train.py --config config.yaml --find-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "175f13f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting training...\n",
      "Training will save results to the output directory specified in config.yaml.\n",
      "You can monitor progress in the output below.\n",
      "🚀 Starting CADI AI Model Training\n",
      "==================================================\n",
      "💻 System Information:\n",
      "  Python: 3.11.13\n",
      "  PyTorch: 2.6.0+cu124\n",
      "🚀 Starting CADI AI Model Training\n",
      "==================================================\n",
      "💻 System Information:\n",
      "  Python: 3.11.13\n",
      "  PyTorch: 2.6.0+cu124\n",
      "  CUDA available: True\n",
      "  GPU: Tesla P100-PCIE-16GB\n",
      "  GPU Memory: 17.1 GB\n",
      "\n",
      "📁 Validating data paths...\n",
      "  ✅ train: 2813 images\n",
      "  ✅ val: 501 images\n",
      "\n",
      "🔍 Finding optimal batch size...\n",
      "  CUDA available: True\n",
      "  GPU: Tesla P100-PCIE-16GB\n",
      "  GPU Memory: 17.1 GB\n",
      "\n",
      "📁 Validating data paths...\n",
      "  ✅ train: 2813 images\n",
      "  ✅ val: 501 images\n",
      "\n",
      "🔍 Finding optimal batch size...\n",
      "  Testing batch size: 32\n",
      "  Testing batch size: 32\n",
      "Ultralytics 8.3.174 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "Ultralytics 8.3.174 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=0, perspective=0.0, plots=False, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=0, perspective=0.0, plots=False, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=runs/detect/train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   1413337  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   1413337  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
      "YOLO11m summary: 231 layers, 20,055,321 parameters, 20,055,305 gradients, 68.2 GFLOPs\n",
      "\n",
      "Transferred 643/649 items from pretrained weights\n",
      "YOLO11m summary: 231 layers, 20,055,321 parameters, 20,055,305 gradients, 68.2 GFLOPs\n",
      "\n",
      "Transferred 643/649 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 1.5±2.0 ms, read: 87.6±63.3 MB/s, size: 100.5 KB)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_datas\u001b[0m\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 1.5±2.0 ms, read: 87.6±63.3 MB/s, size: 100.5 KB)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_datas\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_datas\u001b[0m\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/train is not writeable, cache not saved.\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/train is not writeable, cache not saved.\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.8±0.4 ms, read: 156.6±39.7 MB/s, size: 97.0 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset\u001b[0m\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.8±0.4 ms, read: 156.6±39.7 MB/s, size: 97.0 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset\u001b[0m\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/valid is not writeable, cache not saved.\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/valid is not writeable, cache not saved.\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/88 [00:00<?, ?it/s]Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "        1/1      15.4G      2.454      3.584      1.828         98        640: 1\n",
      "        1/1      15.4G      2.454      3.584      1.828         98        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        501       1715    0.00977     0.0181     0.0056    0.00155\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        501       1715    0.00977     0.0181     0.0056    0.00155\n",
      "\n",
      "1 epochs completed in 0.038 hours.\n",
      "\n",
      "1 epochs completed in 0.038 hours.\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 40.5MB\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 40.5MB\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 40.5MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics 8.3.174 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 40.5MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics 8.3.174 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "YOLO11m summary (fused): 125 layers, 20,032,345 parameters, 0 gradients, 67.7 GFLOPs\n",
      "YOLO11m summary (fused): 125 layers, 20,032,345 parameters, 0 gradients, 67.7 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        501       1715     0.0119     0.0168    0.00613    0.00159\n",
      "Speed: 0.2ms preprocess, 12.1ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "                   all        501       1715     0.0119     0.0168    0.00613    0.00159\n",
      "Speed: 0.2ms preprocess, 12.1ms inference, 0.0ms loss, 2.6ms postprocess per image\n",
      "✅ Batch size 32 works!\n",
      "✅ Batch size 32 works!\n",
      "🎯 Using optimal batch size: 32\n",
      "🤖 Loading model: yolo11m.pt\n",
      "🎯 Using optimal batch size: 32\n",
      "🤖 Loading model: yolo11m.pt\n",
      "🏋️ Starting training...\n",
      "🏋️ Starting training...\n",
      "Ultralytics 8.3.174 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=cadi-training-2508, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/cadi-training-2508, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "Ultralytics 8.3.174 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=32, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11m.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=cadi-training-2508, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/cadi-training-2508, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  1    111872  ultralytics.nn.modules.block.C3k2            [128, 256, 1, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  4                  -1  1    444928  ultralytics.nn.modules.block.C3k2            [256, 512, 1, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1    542720  ultralytics.nn.modules.block.C3k2            [1024, 256, 1, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 19                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1   1642496  ultralytics.nn.modules.block.C3k2            [1024, 512, 1, True]          \n",
      " 23        [16, 19, 22]  1   1413337  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
      " 23        [16, 19, 22]  1   1413337  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
      "YOLO11m summary: 231 layers, 20,055,321 parameters, 20,055,305 gradients, 68.2 GFLOPs\n",
      "\n",
      "YOLO11m summary: 231 layers, 20,055,321 parameters, 20,055,305 gradients, 68.2 GFLOPs\n",
      "\n",
      "Transferred 643/649 items from pretrained weights\n",
      "Transferred 643/649 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.2±0.2 ms, read: 191.3±32.8 MB/s, size: 100.5 KB)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_datas\u001b[0m\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.2±0.2 ms, read: 191.3±32.8 MB/s, size: 100.5 KB)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_datas\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_datas\u001b[0m\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/train is not writeable, cache not saved.\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/train is not writeable, cache not saved.\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 115.5±43.9 MB/s, size: 97.0 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset\u001b[0m\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 115.5±43.9 MB/s, size: 97.0 KB)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset\u001b[0m\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/valid is not writeable, cache not saved.\n",
      "WARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/cadi-ai-retraining/combined_dataset/combined_dataset/valid is not writeable, cache not saved.\n",
      "Plotting labels to runs/cadi-training-2508/labels.jpg... \n",
      "Plotting labels to runs/cadi-training-2508/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/cadi-training-2508\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/88 [00:00<?, ?it/s]\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 106 weight(decay=0.0), 113 weight(decay=0.0005), 112 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/cadi-training-2508\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      1/100      15.5G      2.454      3.584      1.828         98        640: 1\n",
      "      1/100      15.5G      2.454      3.584      1.828         98        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        501       1715    0.00977     0.0181     0.0056    0.00155\n",
      "                   all        501       1715    0.00977     0.0181     0.0056    0.00155\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/88 [00:00<?, ?it/s]\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      2/100      15.1G      2.467      3.094       1.83        138        640: 1\n",
      "      2/100      15.1G      2.467      3.094       1.83        138        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        501       1715     0.0541      0.128     0.0259    0.00674\n",
      "                   all        501       1715     0.0541      0.128     0.0259    0.00674\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/88 [00:00<?, ?it/s]\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      3/100      14.9G      2.426      3.058      1.779        140        640:  WARNING ⚠️ CUDA OutOfMemoryError in TaskAlignedAssigner, using CPU\n",
      "WARNING ⚠️ CUDA OutOfMemoryError in TaskAlignedAssigner, using CPU\n",
      "      3/100      15.4G      2.427      2.962       1.79        132        640: 1\n",
      "      3/100      15.4G      2.427      2.962       1.79        132        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        501       1715      0.381     0.0755     0.0261    0.00786\n",
      "                   all        501       1715      0.381     0.0755     0.0261    0.00786\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/88 [00:00<?, ?it/s]\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      4/100      15.4G      2.419      2.957      1.809        249        640:  "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'https://kkb-production.jupyter-proxy.kaggle.net/k/254158471/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2IiwidHlwIjoiSldUIn0..a0s9QWJMmBZp6RAkOMd8AQ.2t1RyungVdSeT2RwOF4hMTT8gWbjof_ZtfBzyKDBuTgs7U8zIzu7xt3DSBP6UuuCKSSvGedB9ILkK1ZSocRfZgq3-eT_cl0qHBtkHyeuTAUJLAJZQUwvuewlQQqhYs2mpgkXx3M0tSVQIb5-SYVvUH-_YfEzTONEdOWoX2EOi7xkAKpJFxVl1Xo7bZ9PsDMaKvmYlRC9zwzlP4zk60NRcAysOwtEKdOAanAX8ihwtvf-HJLy0xf3hQdpad9x1hjr.I4nTbTM8knPGxjy35rBfng/proxy'. Verify the server is running and reachable."
     ]
    }
   ],
   "source": [
    "# Step 3: Start training with optimal settings\n",
    "# The `train.py` script will automatically use the settings from `config.yaml`\n",
    "# including the auto-detected batch size if you set it to \"auto\" in the config.\n",
    "\n",
    "print(\"🚀 Starting training...\")\n",
    "print(\"Training will save results to the output directory specified in config.yaml.\")\n",
    "print(\"You can monitor progress in the output below.\")\n",
    "\n",
    "!python train.py --config config.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30d1a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate the trained model\n",
    "import glob\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Find the best model weights from the runs directory specified in config.yaml\n",
    "output_dir = 'runs' # Or read from config.yaml\n",
    "with open(\"config.yaml\", 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    output_dir = config.get('output_dir', 'runs')\n",
    "\n",
    "weight_files = glob.glob(os.path.join(output_dir, '**/weights/best.pt'), recursive=True)\n",
    "\n",
    "if weight_files:\n",
    "    best_weights = weight_files[0]  # Get the most recent\n",
    "    print(f\"📊 Evaluating model: {best_weights}\")\n",
    "    \n",
    "    # Load and validate the model\n",
    "    model = YOLO(best_weights)\n",
    "    \n",
    "    # Run validation\n",
    "    print(\"\\n🧪 Running validation...\")\n",
    "    val_results = model.val(data=os.path.join(WORKING_DIR, 'data.yaml'))\n",
    "    \n",
    "else:\n",
    "    print(\"❌ No trained model weights found. Please run training first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
